{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from dblputils import IOUtil, CitationDataset\n",
    "from dblpfeatures import DataUtil\n",
    "from namedisambiguation_v2 import AuthorNameDisambiguation, MergeInfo\n",
    "from citerank import CitationGraph, CiteRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "version = \"v11_reduced\"\n",
    "# the name of the dataset\n",
    "DATA_NAME = \"./datasets/\"+version+\"/dblp_papers_\"+version+\".txt\"\n",
    "# a dataset which represents authors instead of publications\n",
    "AUTHORS = \"./datasets/\"+version+\"/dblp_authors_\"+version+\".txt\"\n",
    "# as 'AUTHORS', but after AuthorNameDisambiguation\n",
    "MERGED = \"./datasets/\"+version+\"/dblp_authors_merged_\"+version+\".txt\"\n",
    "\n",
    "# data structures\n",
    "# simple file which contains all the publication ids and the list of their authors\n",
    "CO_AUTH = \"./datasets/\"+version+\"/data_structures/pub_ids_auths.txt\"\n",
    "# simple file which contains all the author ids and names\n",
    "AUTHS_DICT = \"./datasets/\"+version+\"/data_structures/auths_dict.txt\"\n",
    "# simple file which contains all the years that appear in the dataset and an ordinal number that represent each of them\n",
    "YEARS_DICT = \"./datasets/\"+version+\"/data_structures/years_dict.txt\"\n",
    "# simple file which contains all the raw venues that appear in the dataset and an ordinal number that represent each of them\n",
    "VENUES_DICT = \"./datasets/\"+version+\"/data_structures/venues_dict.txt\"\n",
    "# list of titles translated in english and cleared with clear_text() (title i is referred to publication i)\n",
    "TITLES = \"./datasets/\"+version+\"/data_structures/titles.txt\"\n",
    "# simple file which contains the list of ids of all publications for each author\n",
    "PUBS = \"./datasets/\"+version+\"/data_structures/auth_pubs.txt\"\n",
    "\n",
    "# author name disambiguation\n",
    "nd_version = \"v2\"\n",
    "# sparse matrix (author-publication) where a_ij=1 if the author i wrote the publication j, a_ij=0 otherwise\n",
    "STEP1_Map = \"./datasets/\"+version+\"/name_disambiguation/\"+nd_version+\"/step1_Map.npz\"\n",
    "# sparse matrix containing the similarity value based on meta-path APAPA for each pair of authors\n",
    "STEP1_Mapapa = \"./datasets/\"+version+\"/name_disambiguation/\"+nd_version+\"/step1_Mapapa.npz\"\n",
    "# sparse matrix containing the similarity value based on meta-path APYPA for each pair of authors\n",
    "STEP1_May = \"./datasets/\"+version+\"/name_disambiguation/\"+nd_version+\"/step1_May.npz\"\n",
    "# sparse matrix containing the similarity value based on meta-path APTPA for each pair of authors\n",
    "STEP1_Mat = \"./datasets/\"+version+\"/name_disambiguation/\"+nd_version+\"/step1_Mat.npz\"\n",
    "# sparse matrix containing the similarity value based on meta-path AVA for each pair of authors\n",
    "STEP1_Mav = \"./datasets/\"+version+\"/name_disambiguation/\"+nd_version+\"/step1_Mav.npz\"\n",
    "# contains the diagonal of the matrix obtained computing the dot product between Mav and Mav.T\n",
    "STEP1_Mav_Diag = \"./datasets/\"+version+\"/name_disambiguation/\"+nd_version+\"/step1_Mav_Diagonal.npy\"\n",
    "# sparse matrix containing the final similarity value of each pair of authors\n",
    "STEP1_SIM = \"./datasets/\"+version+\"/name_disambiguation/\"+nd_version+\"/step1_Sims.npz\"\n",
    "# contains a key for each author that has at least one possible duplicate (resulting from step 1); the value is a list of \n",
    "# pairs <id,similarity_value> (one for each of possible duplicates of the key)\n",
    "STEP2_SIM = \"./datasets/\"+version+\"/name_disambiguation/\"+nd_version+\"/step2_similarities.txt\"\n",
    "# contains a row for each author with matched duplicates; each row is formed by an author's id and the list of the matched\n",
    "# authors' ids\n",
    "STEP3_DUPL = \"./datasets/\"+version+\"/name_disambiguation/\"+nd_version+\"/step3_duplicates.txt\"\n",
    "# dictionary where for each <key,value> pair the key is an author id from the complete authors dataset and the value\n",
    "# is the same id if he's no one duplicate or his duplicate's id if he's someone duplicate\n",
    "MERGED_IDMAP = \"./datasets/\"+version+\"/name_disambiguation/\"+nd_version+\"/step3_idmap.txt\"\n",
    "\n",
    "# CiteRank\n",
    "cg_version = \"v1\"\n",
    "# a[i,j]=1 if a link exists from i to j, 0 otherwise (for each i and j)\n",
    "ADJ_MATRIX = \"./datasets/\"+version+\"/citations_graph/\"+cg_version+\"/adjacency_matrix.npz\"\n",
    "# a[i,j]=k where k is the number of collaborations between i and j (for each i and j)\n",
    "COL_MATRIX = \"./datasets/\"+version+\"/citations_graph/\"+cg_version+\"/collaboration_matrix.npz\"\n",
    "# a[i,j]=k where k is the number of existing links from i to j (for each i and j)\n",
    "CIT_MATRIX = \"./datasets/\"+version+\"/citations_graph/\"+cg_version+\"/citation_matrix.npz\"\n",
    "Wcol_MATRIX = \"./datasets/\"+version+\"/citations_graph/\"+cg_version+\"/Wco_matrix.npz\"\n",
    "Wcit_MATRIX = \"./datasets/\"+version+\"/citations_graph/\"+cg_version+\"/Wcit_matrix.npz\"\n",
    "Wlp_MATRIX = \"./datasets/\"+version+\"/citations_graph/\"+cg_version+\"/Wlp_matrix.npz\"\n",
    "Worg_MATRIX = \"./datasets/\"+version+\"/citations_graph/\"+cg_version+\"/Worg_matrix.npz\"\n",
    "W_MATRIX_LOC = \"./datasets/\"+version+\"/citations_graph/\"+cg_version+\"/weight_matrices/\"\n",
    "RANKS_LOC = \"./datasets/\"+version+\"/citations_graph/\"+cg_version+\"/ranks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the fields of a json object loaded from the dataset\n",
    "all_columns = np.array([\"id\", \"title\", \"authors\", \"venue\", \"year\", \"keywords\", \"fos\", \"references\",\n",
    "                        \"n_citation\", \"page_start\", \"page_end\", \"doc_type\", \"lang\", \"publisher\",\n",
    "                        \"volume\", \"issue\", \"issn\", \"isbn\", \"doi\", \"pdf\", \"url\", \"abstract\", \"indexed_abstract\"])\n",
    "# Reduced number of fields of a json objed loaded from the dataset\n",
    "reduced_columns = np.array([\"id\", \"title\", \"authors\", \"venue\", \"year\", \"references\"])\n",
    "# Columns of the authors dataset\n",
    "author_columns = np.array([\"id\", \"name\", \"org\", \"pubs\"])\n",
    "# authors['pubs'] inner keys\n",
    "author_pub_columns = np.array([\"id\", \"title\", \"year\", \"venue\", \"references\"])\n",
    "# total number of entries in the original dataset\n",
    "dataset_lines = 1425148      #4107340\n",
    "# total number of entries in the authors dataset\n",
    "authors_lines = 1145383      #3655052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of some classes\n",
    "ioutil = IOUtil()\n",
    "datautil = DataUtil()\n",
    "authorNameDisambiguation = AuthorNameDisambiguation(ioutil)\n",
    "mergeinfo = MergeInfo(ioutil)\n",
    "citegraph = CitationGraph()\n",
    "citerank = CiteRank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = CitationDataset(\"./datasets/v11/dblp_papers_v11.txt\", 4107340)\n",
    "# reducing the number of columns in the whole dataset\n",
    "ioutil.dumpLinesFromJson(publications, \"./datasets/v11/dblp_papers_v11_less-cols.txt\", reduced_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = np.array(ioutil.loadAsJson(\"./datasets/v11/dblp_papers_v11_less-cols.txt\"))\n",
    "# selecting 300000 random publications from the dataset\n",
    "ioutil.selectRandomPro(publications, 300000, DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_keys = [\"name\", \"org\"]\n",
    "pub_keys = [\"title\", \"year\"]\n",
    "pub_nested = {\"venue\":\"raw\"}\n",
    "# Authors dict structure:\n",
    "# dict := {<author_id>:<author_info>}\n",
    "#   <author_id> := str\n",
    "#   <author_info> := {\"id\":str, \"name\":str, \"org\":str, \"pubs\":list<pub_info>}\n",
    "#     <pub_info> := {\"id\":str, \"title\":str, \"year\":str, \"venue\":str, \"references\":list<str>}\n",
    "# extracting a dataset representation which focuses on authors instead of publications\n",
    "publications = CitationDataset(DATA_NAME, dataset_lines)\n",
    "authors = ioutil.extractAuthorsDataset(publications, auth_keys, pub_keys, pub_nested)\n",
    "ioutil.dumpLinesFromJson(list(authors.values()), AUTHORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = CitationDataset(AUTHORS, authors_lines)\n",
    "publications = CitationDataset(DATA_NAME, dataset_lines)\n",
    "datautil.extractAuthors(authors, save=True, filename=AUTHS_DICT)\n",
    "datautil.computeAuthorsInd()\n",
    "datautil.extractCoAuthors(publications, save=True, filename=CO_AUTH)\n",
    "datautil.computePublicationsInd()\n",
    "datautil.extractPublications(authors, save=True, filename=PUBS)\n",
    "datautil.extractYears(publications, save=True, filename=YEARS_DICT)\n",
    "datautil.extractVenues(publications, save=True, filename=VENUES_DICT)\n",
    "specials = datautil.extractTitles(publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referring to reduces dataset, may change if the dataset changes\n",
    "import langid\n",
    "from googletrans import Translator\n",
    "lang_predictor = langid.classify\n",
    "lang_translator = Translator()\n",
    "\n",
    "not_en = []\n",
    "for i in range(len(specials)):\n",
    "    if(lang_predictor(specials[i])[0]!='en'):\n",
    "        not_en.append(i)\n",
    "    if(i%10==0):\n",
    "        print(\"\\Classified: %d/%d\"%(i,len(specials)),end='',flush=True)\n",
    "print(\"\\Classified: %d/%d\"%(i+1,len(specials)))\n",
    "en = list(set(np.arange(len(specials)))-set(not_en))\n",
    "done = 0\n",
    "for i in en:\n",
    "    specials[i] = datautil.clear_title(specials[i])\n",
    "    if(done%100==0):\n",
    "        print(\"\\rCorrected: %d/%d\"%(done,len(en)),end='',flush=True)\n",
    "    done += 1\n",
    "print(\"\\rCorrected: %d/%d\"%(done,len(en)))\n",
    "done = 0\n",
    "for i in not_en:\n",
    "    if(done<3622):\n",
    "        done += 1\n",
    "        continue\n",
    "    specials[i] = datautil.clear_title(lang_translator.translate(specials[i]).text)\n",
    "    if(done%100==0):\n",
    "        print(\"\\rCorrected: %d/%d\"%(done,len(not_en)),end='',flush=True)\n",
    "    done += 1\n",
    "print(\"\\rCorrected: %d/%d\"%(done,len(not_en)))\n",
    "copied = 0\n",
    "for i in range(len(datautil.titles_list)):\n",
    "    if(datautil.titles_list[i]==\"+\"):\n",
    "        datautil.titles_list[i] = specials[copied]\n",
    "        copied += 1\n",
    "    if(i%10000==0):\n",
    "        print(\"\\rSecured: %d/%d\"%(i,len(datautil.titles_list)),end='',flush=True)\n",
    "print(\"\\rSecured: %d/%d\"%(i+1,len(datautil.titles_list)))\n",
    "with open(TITLES, 'w', encoding='utf-8') as file:\n",
    "    for i in range(len(datautil.titles_list)):\n",
    "        file.write(\"%s%s\"%(datautil.titles_list[i],(\"\" if i==len(datautil.titles_list)-1 else \"\\n\")))\n",
    "        if(i%10000==0):\n",
    "            print(\"\\rLines written: %d/%d\"%(i,len(datautil.titles_list)),end='',flush=True)\n",
    "    print(\"\\rLines written: %d/%d\"%(i+1,len(datautil.titles_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Disambiguation Problem\n",
    "src: Ranking-Based Name Matching for Author Disambiguation in Bibliographic Data<br>\n",
    "readaptation: 3 steps -> p-step, r-step, merge-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***r-step***: *Improving the recall. In order to accomplish this task is used the idea of meta-paths [AVA (same venue), APAPA (co-authors of my co-authors), APTPA (title similarities), APYPA (same year)].*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.0: computing the adjacency matrix Map (author-publication) where a_ij=1 if the author i wrote the \n",
    "# publication j, a_ij=0 otherwise\n",
    "authorNameDisambiguation.computeMap(datautil.auth_id_ind, datautil.pub_id_ind, datautil.publications_dict, filename=STEP1_Map)\n",
    "# Step 1.1: computing meta-path APAPA for each author (applying l2-norm)\n",
    "authorNameDisambiguation.computeMapapa(filename=STEP1_Mapapa)\n",
    "# Step 1.2: computing meta-path APYPA for each author (applying l2-norm)\n",
    "authorNameDisambiguation.computeMay(CitationDataset(DATA_Name, dataset_lines), datautil.pub_id_ind, datautil.years_dict, \n",
    "                                    filename=STEP1_May)\n",
    "# Step 1.3: computing meta-path APTPA for each author (applying l2-norm)\n",
    "authorNameDisambiguation.computeMat(datautil.titles_list, filename=STEP1_Mat)\n",
    "# Step 1.4: computing meta-path AVA for each author (applying PathSim)\n",
    "authorNameDisambiguation.computeMav(CitationDataset(AUTHORS, authors_lines), datautil.venues_dict, datautil.auth_id_ind, \n",
    "                                    filename=STEP1_Mav)\n",
    "# Step 1.5: computing the diagonal of the dot product between Mav and Mav.T\n",
    "authorNameDisambiguation.computeMavDiagonal(filename=STEP1_Mav_Diag)\n",
    "# Step 1.6: computing the similarity value based on meta-paths for each pair of authors\n",
    "authorNameDisambiguation.r_step(np.array([1/4]*4), filename=STEP1_SIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***p-step***: *Improving the precision; given an author ID and the previous results, one should find its real duplicates via String-based consideration.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: for each author that passed the p_step, compute the string based similarity value with his potential duplicates\n",
    "authorNameDisambiguation.p_step(datautil.authors_dict, STEP2_SIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***m-step***: *Merging the resulting duplicates, based on the similarity values obtained in the r-step and the p-step.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: finding the actual duplicates for each author that passed the first step, i.e. the possible duplicate authors \n",
    "# with a similarity value (string based) over a threshold\n",
    "authorNameDisambiguation.m_step(filename=STEP3_DUPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataset with the merged authors\n",
    "authors = ioutil.loadAsJson(AUTHORS)\n",
    "tmp = dict()\n",
    "for entry in authors:\n",
    "    tmp[entry['id']] = entry\n",
    "authors = tmp\n",
    "del tmp\n",
    "mergeinfo.computeMergeDataset(authors, datautil.authors_pubs, authorNameDisambiguation.duplicates, filename=MERGED)\n",
    "mergeinfo.computeIdMap(CitationDataset(AUTHORS, authors_lines), authorNameDisambiguation.duplicates, filename=MERGED_IDMAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the citation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = CitationDataset(MERGED, 1142584)\n",
    "datautil.auth_id_ind = dict()\n",
    "i = 0\n",
    "for author in authors:\n",
    "    datautil.auth_id_ind[author['id']] = i\n",
    "    if(i%10000==0):\n",
    "        print(\"\\rExamined: %d/%d\"%(i,len(authors)),end='',flush=True)\n",
    "    i += 1\n",
    "print(\"\\rExamined: %d/%d\"%(i,len(authors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citegraph.adjacencyMatrix(authors, datautil.auth_id_ind, datautil.publications_dict, mergeinfo.idMap, filename=ADJ_MATRIX)\n",
    "citegraph.collaborationMatrix(authors, datautil.auth_id_ind, datautil.publications_dict, mergeinfo.idMap, filename=COL_MATRIX)\n",
    "citegraph.citationMatrix(authors, datautil.auth_id_ind, datautil.publications_dict, mergeinfo.idMap, filename=CIT_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = CitationDataset(MERGED, 1142584)\n",
    "orgs_list = np.array([None]*len(authors))\n",
    "i = 0\n",
    "for author in authors:\n",
    "    orgs_list[i] = author['org']\n",
    "    if(i%10000==0):\n",
    "        print(\"\\rCompleted: %d/%d\"%(i,len(authors)),end='',flush=True)\n",
    "    i += 1\n",
    "print(\"\\rCompleted: %d/%d\"%(i,len(authors)))\n",
    "orgs_values = [orgs_list[orgs_list==''].shape[0]/orgs_list.shape[0], orgs_list[orgs_list!=''].shape[0]/orgs_list.shape[0]]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(orgs_values, normalize=False, labels=[\"%.2f%s\"%(orgs_values[i]*100,\"%\") for i in range(len(orgs_values))],\n",
    "       explode=(0.0, 0.1))\n",
    "plt.legend(['Missing values', 'Valid values'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citegraph.weight_collaborations(filename=Wcol_MATRIX)\n",
    "citegraph.weight_citations2(filename=Wcit_MATRIX)\n",
    "citegraph.weight_2loops(filename=Wlp_MATRIX)\n",
    "citegraph.weight_orgs(authors, sigma=0.75, filename=Worg_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_col_only = citegraph.sum_weight_matrices(np.array([1,0,0,0]).astype(np.float), filename=W_MATRIX_LOC+\"wam_(1,0,0,0).npz\")\n",
    "W_cit_only = citegraph.sum_weight_matrices(np.array([0,1,0,0]).astype(np.float), filename=W_MATRIX_LOC+\"wam_(0,1,0,0).npz\")\n",
    "W_lp_only  = citegraph.sum_weight_matrices(np.array([0,0,1,0]).astype(np.float), filename=W_MATRIX_LOC+\"wam_(0,0,1,0).npz\")\n",
    "W_org_only = citegraph.sum_weight_matrices(np.array([0,0,0,1]).astype(np.float), filename=W_MATRIX_LOC+\"wam_(0,0,0,1).npz\")\n",
    "W_uniform = citegraph.sum_weight_matrices(np.array([1/4]*4), filename=W_MATRIX_LOC+\"wam_(0.25,0.25,0.25,0.25).npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CiteRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing values for alpha**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = citerank.pagerank(citegraph.Cit, tol=1.0e-8)\n",
    "r_uniform = citerank.pagerank(W_uniform.multiply(citegraph.Cit), tol=1.0e-8)\n",
    "r_col_only = citerank.pagerank(W_col_only.multiply(citegraph.Cit), tol=1.0e-8)\n",
    "r_cit_only = citerank.pagerank(W_cit_only.multiply(citegraph.Cit), tol=1.0e-8)\n",
    "r_lp_only  = citerank.pagerank(W_lp_only.multiply(citegraph.Cit), tol=1.0e-8)\n",
    "r_org_only = citerank.pagerank(W_org_only.multiply(citegraph.Cit), tol=1.0e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = max(r.max(), r_uniform.max(), r_col_only.max(), r_cit_only.max(), r_lp_only.max(), r_org_only.max())\n",
    "delta = 0.0001\n",
    "\n",
    "_, axes = plt.subplots(3, 2, figsize=(17,20))\n",
    "\n",
    "axes[0][0].plot(r, r_uniform, 'o', markersize=2, alpha=0.7, label=\"Uniform distribution\")\n",
    "axes[0][0].plot(r, r_col_only, 'o', markersize=2, alpha=0.7, label=\"Collaboration weights only\")\n",
    "axes[0][0].plot(r, r_cit_only, 'o', markersize=2, alpha=0.7, label=\"Citation weights only\")\n",
    "axes[0][0].plot(r, r_lp_only, 'o', markersize=2, alpha=0.7, label=\"2-loops weights only\")\n",
    "axes[0][0].plot(r, r_org_only, 'o', markersize=2, alpha=0.7, label=\"Organization weights only\")\n",
    "axes[0][0].set_xticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[0][0].set_yticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[0][0].set_title(\"Relation between PageRank and CiteRank with different beta values\")\n",
    "axes[0][0].set_xlabel(\"PageRank\")\n",
    "axes[0][0].set_ylabel(\"CiteRank\")\n",
    "axes[0][0].legend()\n",
    "axes[0][0].grid()\n",
    "\n",
    "axes[0][1].plot(r, r_uniform, 'o', markersize=2, alpha=0.5)\n",
    "axes[0][1].set_xticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[0][1].set_yticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[0][1].set_title(\"CiteRank with uniform distribution of beta values\")\n",
    "axes[0][1].set_xlabel(\"PageRank\")\n",
    "axes[0][1].set_ylabel(\"CiteRank\")\n",
    "axes[0][1].grid()\n",
    "\n",
    "axes[1][0].plot(r, r_col_only, 'o', markersize=2, alpha=0.5)\n",
    "axes[1][0].set_xticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[1][0].set_yticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[1][0].set_title(\"CiteRank with only collaboration weight beta set to 1.0\")\n",
    "axes[1][0].set_xlabel(\"PageRank\")\n",
    "axes[1][0].set_ylabel(\"CiteRank\")\n",
    "axes[1][0].grid()\n",
    "\n",
    "axes[1][1].plot(r, r_cit_only, 'o', markersize=2, alpha=0.5)\n",
    "axes[1][1].set_xticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[1][1].set_yticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[1][1].set_title(\"CiteRank with only citation weight beta set to 1.0\")\n",
    "axes[1][1].set_xlabel(\"PageRank\")\n",
    "axes[1][1].set_ylabel(\"CiteRank\")\n",
    "axes[1][1].grid()\n",
    "\n",
    "axes[2][0].plot(r, r_lp_only, 'o', markersize=2, alpha=0.5)\n",
    "axes[2][0].set_xticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[2][0].set_yticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[2][0].set_title(\"CiteRank with only 2-loops weight beta set to 1.0\")\n",
    "axes[2][0].set_xlabel(\"PageRank\")\n",
    "axes[2][0].set_ylabel(\"CiteRank\")\n",
    "axes[2][0].grid()\n",
    "\n",
    "axes[2][1].plot(r, r_org_only, 'o', markersize=2, alpha=0.5)\n",
    "axes[2][1].set_xticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[2][1].set_yticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[2][1].set_title(\"CiteRank with only organization weight beta set to 1.0\")\n",
    "axes[2][1].set_xlabel(\"PageRank\")\n",
    "axes[2][1].set_ylabel(\"CiteRank\")\n",
    "axes[2][1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(2, 2, figsize=(15,15))\n",
    "\n",
    "axes[0][0].plot(r, r0, 'o', markersize=2, alpha=0.5)\n",
    "axes[0][0].set_xticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[0][0].set_yticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[0][0].set_title(\"beta = [0.12, 0.28, 0.4, 0.2]\")\n",
    "axes[0][0].set_xlabel(\"PageRank\")\n",
    "axes[0][0].set_ylabel(\"CiteRank\")\n",
    "axes[0][0].grid()\n",
    "\n",
    "axes[0][1].plot(r, r1, 'o', markersize=2, alpha=0.5)\n",
    "axes[0][1].set_xticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[0][1].set_yticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[0][1].set_title(\"beta = [0.12, 0.23, 0.45, 0.2]\")\n",
    "axes[0][1].set_xlabel(\"PageRank\")\n",
    "axes[0][1].set_ylabel(\"CiteRank\")\n",
    "axes[0][1].grid()\n",
    "\n",
    "axes[1][0].plot(r, r2, 'o', markersize=2, alpha=0.5)\n",
    "axes[1][0].set_xticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[1][0].set_yticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[1][0].set_title(\"beta = [0.05, 0.15, 0.6, 0.2]\")\n",
    "axes[1][0].set_xlabel(\"PageRank\")\n",
    "axes[1][0].set_ylabel(\"CiteRank\")\n",
    "axes[1][0].grid()\n",
    "\n",
    "axes[1][1].plot(r, r3, 'o', markersize=2, alpha=0.5)\n",
    "axes[1][1].set_xticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[1][1].set_yticks(np.arange(0.0, max_val+delta, delta))\n",
    "axes[1][1].set_title(\"beta = [0.05, 0.3, 0.6, 0.05]\")\n",
    "axes[1][1].set_xlabel(\"PageRank\")\n",
    "axes[1][1].set_ylabel(\"CiteRank\")\n",
    "axes[1][1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W3\n",
    "cr = r3\n",
    "sparse.save_npz(W_MATRIX_LOC+\"wam_(0.05,0.3,0.6,0.05).npz\", W)\n",
    "np.save(RANKS_LOC+\"citerank.npy\", cr)\n",
    "np.save(RANKS_LOC+\"pagerank.npy\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing the score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = CitationDataset(MERGED, 1142584)\n",
    "ids = np.array([None]*len(authors))\n",
    "i = 0\n",
    "for author in authors:\n",
    "    ids[i] = author['id']\n",
    "    if(i%10000==0):\n",
    "        print(\"\\rExamined: %d/%d\"%(i,len(authors)),end='',flush=True)\n",
    "    i += 1\n",
    "print(\"\\rExamined: %d/%d\"%(i,len(authors)))\n",
    "\n",
    "ranks_r, ranks_cr, diffs = citerank.sorted_ranks(np.array([r, cr]), ids, diffs=True)\n",
    "received_cits = np.array(citegraph.Cit.sum(axis=0)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_score = citerank.minMaxScaler(np.log10(r) - np.log10(cr))\n",
    "\n",
    "pos_score = citerank.minMaxScaler(diffs)\n",
    "\n",
    "z_score = citerank.minMaxScaler(zscore(cr) - zscore(r))\n",
    "\n",
    "# original formula: np.log10(np.sqrt(np.abs(zscore_r*zscore_cr)))+(1-ranks_r/len(ranks_r))+(1-ranks_cr/len(ranks_cr))\n",
    "s = (1/2)*np.log10(np.abs(zscore(r)*zscore(cr)))+(2-(ranks_r+ranks_cr)/len(ranks_r))\n",
    "s = citerank.minMaxScaler(s)\n",
    "\n",
    "#s2 = citerank.minMaxScaler(np.log10(np.abs(zscore_cr-zscore_r))+(2-(ranks_r+ranks_cr)/len(ranks_r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(2, 2, figsize=(15,15))\n",
    "\n",
    "axes[0][0].plot(received_cits, log_score, 'o', alpha=0.5)\n",
    "axes[0][0].plot([0, received_cits.max()], [log_score.mean(), log_score.mean()], 'r')\n",
    "axes[0][0].plot([0, received_cits.max()], [log_score.mean()+log_score.std(), log_score.mean()+log_score.std()], 'g--')\n",
    "axes[0][0].plot([0, received_cits.max()], [log_score.mean()-log_score.std(), log_score.mean()-log_score.std()], 'g--')\n",
    "axes[0][0].annotate(\"%d points\"%log_score[(log_score>=log_score.mean()-log_score.std())*\n",
    "                                   (log_score<=log_score.mean()+log_score.std())].shape[0],\n",
    "             xy=(50000, log_score.mean()+log_score.std()-0.02), xytext=(50000, 0.3), arrowprops={'arrowstyle':'->'})\n",
    "axes[0][0].set_title(\"Score = log10(PageRank) - log10(CiteRank)\")\n",
    "axes[0][0].set_xlabel(\"n째 of received citations\")\n",
    "axes[0][0].set_ylabel(\"Score\")\n",
    "\n",
    "axes[0][1].plot(received_cits, pos_score, 'o', alpha=0.5)\n",
    "axes[0][1].plot([0, received_cits.max()], [pos_score.mean(), pos_score.mean()], 'r')\n",
    "axes[0][1].plot([0, received_cits.max()], [pos_score.mean()+pos_score.std(), pos_score.mean()+pos_score.std()], 'g--')\n",
    "axes[0][1].plot([0, received_cits.max()], [pos_score.mean()-pos_score.std(), pos_score.mean()-pos_score.std()], 'g--')\n",
    "axes[0][1].annotate(\"%d points\"%pos_score[\n",
    "    (pos_score>=pos_score.mean()-pos_score.std())*(pos_score<=pos_score.mean()+pos_score.std())].shape[0], \n",
    "    xy=(50000, pos_score.mean()+pos_score.std()-0.02), xytext=(50000, 0.8), arrowprops={'arrowstyle':'->'})\n",
    "axes[0][1].set_title(\"Score = pos(PageRank) - pos(CiteRank)\")\n",
    "axes[0][1].set_xlabel(\"n째 of received citations\")\n",
    "axes[0][1].set_ylabel(\"Score\")\n",
    "\n",
    "axes[1][0].plot(received_cits, z_score, 'o', alpha=0.5)\n",
    "axes[1][0].plot([0, received_cits.max()], [z_score.mean(), z_score.mean()], 'r--')\n",
    "axes[1][0].plot([0, received_cits.max()], [z_score.mean()+z_score.std(), z_score.mean()+z_score.std()], 'g--')\n",
    "axes[1][0].plot([0, received_cits.max()], [z_score.mean()-z_score.std(), z_score.mean()-z_score.std()], 'g--')\n",
    "axes[1][0].annotate(\"%d points\"%z_score[(z_score>=z_score.mean()-z_score.std())*\n",
    "                                     (z_score<=z_score.mean()+z_score.std())].shape[0],\n",
    "             xy=(50000, z_score.mean()+z_score.std()-0.01), xytext=(50000, 0.5), arrowprops={'arrowstyle':'->'})\n",
    "axes[1][0].set_title(\"Score = zscore(CiteRank) - zscore(PageRank)\")\n",
    "axes[1][0].set_xlabel(\"n째 of received citations\")\n",
    "axes[1][0].set_ylabel(\"Score\")\n",
    "\n",
    "axes[1][1].plot(received_cits, s, 'o', alpha=0.5)\n",
    "axes[1][1].plot([0, received_cits.max()], [s.mean(), s.mean()], 'r--')\n",
    "axes[1][1].plot([0, received_cits.max()], [s.mean()+s.std(), s.mean()+s.std()], 'g--')\n",
    "axes[1][1].plot([0, received_cits.max()], [s.mean()-s.std(), s.mean()-s.std()], 'g--')\n",
    "axes[1][1].annotate(\"%d points\"%s[(s>=s.mean()-s.std())*(s<=s.mean()+s.std())].shape[0],\n",
    "             xy=(50000, s.mean()+s.std()-0.05), xytext=(50000, 0.6), arrowprops={'arrowstyle':'->'})\n",
    "axes[1][1].set_title(\"Score = f(PageRank, CiteRank)\")\n",
    "axes[1][1].set_xlabel(\"n째 of received citations\")\n",
    "axes[1][1].set_ylabel(\"Score\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_unical_0_id = '3821842'\n",
    "prof_unical_0 = np.where(ids==prof_unical_0_id)[0][0]\n",
    "prof_unical_1_id = '184075056'\n",
    "prof_unical_1 = np.where(ids==prof_unical_1_id)[0][0]\n",
    "prof_unical_2_id = '2071564828'\n",
    "prof_unical_2 = np.where(ids==prof_unical_2_id)[0][0]\n",
    "prof_unical_3_id = '2075460159'\n",
    "prof_unical_3 = np.where(ids==prof_unical_3_id)[0][0]\n",
    "prof_unical_4_id = '2294106506'\n",
    "prof_unical_4 = np.where(ids==prof_unical_4_id)[0][0]\n",
    "prof_unical_5_id = '2143117249'\n",
    "prof_unical_5 = np.where(ids==prof_unical_5_id)[0][0]\n",
    "prof_unical_6_id = '2163236697'\n",
    "prof_unical_6 = np.where(ids==prof_unical_6_id)[0][0]\n",
    "prof_unical_7_id = '273425128'\n",
    "prof_unical_7 = np.where(ids==prof_unical_7_id)[0][0]\n",
    "prof_unical_8_id = '1976489361'\n",
    "prof_unical_8 = np.where(ids==prof_unical_8_id)[0][0]\n",
    "first_cls = np.argsort(-cr)[0]\n",
    "second_cls = np.argsort(-cr)[1]\n",
    "last_cls = np.argsort(-cr)[-1]\n",
    "\n",
    "df = pd.DataFrame([[r[prof_unical_0], cr[prof_unical_0], ranks_r[prof_unical_0], ranks_cr[prof_unical_0], \n",
    "                    log_score[prof_unical_0], pos_score[prof_unical_0], z_score[prof_unical_0], s[prof_unical_0]],\n",
    "                   [r[prof_unical_1], cr[prof_unical_1], ranks_r[prof_unical_1], ranks_cr[prof_unical_1],\n",
    "                    log_score[prof_unical_1], pos_score[prof_unical_1], z_score[prof_unical_1], s[prof_unical_1]],\n",
    "                   [r[prof_unical_2], cr[prof_unical_2], ranks_r[prof_unical_2], ranks_cr[prof_unical_2],\n",
    "                    log_score[prof_unical_2], pos_score[prof_unical_2], z_score[prof_unical_2], s[prof_unical_2]],\n",
    "                   [r[prof_unical_3], cr[prof_unical_3], ranks_r[prof_unical_3], ranks_cr[prof_unical_3],\n",
    "                    log_score[prof_unical_3], pos_score[prof_unical_3], z_score[prof_unical_3], s[prof_unical_3]],\n",
    "                   [r[prof_unical_4], cr[prof_unical_4], ranks_r[prof_unical_4], ranks_cr[prof_unical_4],\n",
    "                    log_score[prof_unical_4], pos_score[prof_unical_4], z_score[prof_unical_4], s[prof_unical_4]],\n",
    "                   [r[prof_unical_5], cr[prof_unical_5], ranks_r[prof_unical_5], ranks_cr[prof_unical_5],\n",
    "                    log_score[prof_unical_5], pos_score[prof_unical_5], z_score[prof_unical_5], s[prof_unical_5]],\n",
    "                   [r[prof_unical_6], cr[prof_unical_6], ranks_r[prof_unical_6], ranks_cr[prof_unical_6],\n",
    "                    log_score[prof_unical_6], pos_score[prof_unical_6], z_score[prof_unical_6], s[prof_unical_6]],\n",
    "                   [r[prof_unical_7], cr[prof_unical_7], ranks_r[prof_unical_7], ranks_cr[prof_unical_7],\n",
    "                    log_score[prof_unical_7], pos_score[prof_unical_7], z_score[prof_unical_7], s[prof_unical_7]],\n",
    "                   [r[prof_unical_8], cr[prof_unical_8], ranks_r[prof_unical_8], ranks_cr[prof_unical_8],\n",
    "                    log_score[prof_unical_8], pos_score[prof_unical_8], z_score[prof_unical_8], s[prof_unical_8]],\n",
    "                   [r[first_cls], cr[first_cls], ranks_r[first_cls], ranks_cr[first_cls],\n",
    "                    log_score[first_cls], pos_score[first_cls], z_score[first_cls], s[first_cls]],\n",
    "                   [r[second_cls], cr[second_cls], ranks_r[second_cls], ranks_cr[second_cls],\n",
    "                    log_score[second_cls], pos_score[second_cls], z_score[second_cls], s[second_cls]],\n",
    "                   [r[last_cls], cr[last_cls], ranks_r[last_cls], ranks_cr[last_cls],\n",
    "                    log_score[last_cls], pos_score[last_cls], z_score[last_cls], s[last_cls]]],\n",
    "                  index=['Docente Unical 0', 'Docente Unical 1', 'Docente Unical 2', 'Docente Unical 3',\n",
    "                         'Docente Unical 4', 'Docente Unical 5', 'Docente Unical 6', 'Docente Unical 7',\n",
    "                         'Docente Unical 8', 'First classified', 'Second classified', 'Last classified'], \n",
    "                  columns=['PageRank', 'CiteRank', 'Pos_PageRank', \n",
    "                           'Pos_CiteRank', 'Log_S', 'Pos_S', 'Z_S', 'S'])\n",
    "df.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Searching correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.load(RANKS_LOC+\"scores.npy\")\n",
    "citegraph.loadSelfMatrices(Cit=CIT_MATRIX, Col=COL_MATRIX, A=ADJ_MATRIX)\n",
    "citegraph.computePubsNum(CitationDataset(MERGED, 1142584))\n",
    "received_cits = np.array(citegraph.Cit.sum(axis=0)).flatten()\n",
    "A_loops = citegraph.A.multiply(citegraph.A.T)\n",
    "attitudes = A_loops.getnnz(axis=1)/np.where(citegraph.A.getnnz(axis=1)==0.0, 1.0, citegraph.A.getnnz(axis=1))\n",
    "D = sparse.spdiags(attitudes, 0, *A_loops.shape, format=\"csr\")\n",
    "Tol = A_loops - (D * A_loops)\n",
    "mean_tolerances = np.array(Tol.mean(axis=1)).flatten()\n",
    "\n",
    "data = np.hstack((citegraph.pubs_num.reshape((-1,1)), received_cits.reshape((-1,1)),\n",
    "                  mean_tolerances.reshape((-1,1)), s.reshape((-1,1))))\n",
    "columns = ['Pubs', 'Cits', 'Tol', 'Score']\n",
    "df = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(2, 1, figsize=(9,15))\n",
    "sn.heatmap(df.corr(method='pearson'), annot=True, cmap=sn.color_palette(\"Blues\", as_cmap=True).reversed(), ax=axes[0])\n",
    "sn.heatmap(df.corr(method='spearman'), annot=True, cmap=sn.color_palette(\"Blues\", as_cmap=True).reversed(), ax=axes[1])\n",
    "for i in range(len(axes)):\n",
    "    for tick in axes[i].get_yticklabels():\n",
    "        tick.set_rotation(0)\n",
    "axes[0].set_title(\"Correlation matrix (Pearson's coefficient)\")\n",
    "axes[1].set_title(\"Correlation matrix (Spearman's coefficient)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
